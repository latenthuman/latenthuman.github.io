<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>LatentHuman: Shape-and-Pose Disentangled Latent Representation for Human Bodies</title>
  <!-- Bootstrap -->
  <link href="css/bootstrap-4.4.1.css" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
</head>

<!-- cover -->
<section>
  <div class="jumbotron text-center mt-0">
    <div class="container">
      <div class="row">
        <div class="col-12">
          <h2>LatentHuman: Shape-and-Pose Disentangled Latent Representation <br /> for for Human Bodies</h2>
          <h4 style="color:#5a6268;">3DV 2021</h4>
          <hr>
          <h6>
            <a href="http://sandrolombardi.com/" target="_blank">Sandro Lombardi</a><sup>1</sup>,
            <a href="https://github.com/ybbbbt/" target="_blank">Bangbang Yang</a><sup>2</sup>,
            <a href="https://github.com/purenightmare/" target="_blank">Tianxing Fan</a><sup>2</sup>,
            <a href="http://www.cad.zju.edu.cn/home/bao/" target="_blank">Hujun Bao</a><sup>2</sup>,
            <a href="http://www.cad.zju.edu.cn/home/gfzhang/" target="_blank">Guofeng Zhang</a><sup>2</sup>,
            <a href="https://people.inf.ethz.ch/pomarc/" target="_blank">Marc Pollefeys</a><sup>1 3</sup>,
            <a href="https://zhpcui.github.io/" target="_blank">Zhaopeng Cui</a><sup>2</sup>
          </h6>
          <p>
            <sup>1</sup>ETH Zurich &nbsp;&nbsp;
            <sup>2</sup>State Key Lab of CAD & CG, Zhejiang University &nbsp;&nbsp;
            <sup>3</sup>Microsoft
            <br>

          <div class="row justify-content-center">
            <div class="column">
              <!-- TODO: update to the arXiv paper link. -->
              <p class="mb-5"><a class="btn btn-large btn-light"
                  href="https://arxiv.org/pdf/2111.15113.pdf" role="button"
                  target="_blank">
                  <i class="fa fa-file"></i> Paper</a> </p>
            </div>
            <div class="column">
              <p class="mb-5"><a class="btn btn-large btn-light" href="https://github.com/latenthuman/latenthuman"
                  role="button" target="_blank">
                  <i class="fa fa-github-alt"></i> Code (Comming Soon)</a> </p>
            </div>
            <div class="column">
              <p class="mb-5"><a class="btn btn-large btn-light"
                  href="http://www.cad.zju.edu.cn/home/gfzhang/papers/latent_human/latent_human_supp.pdf" role="button">
                  <i class="fa fa-file"></i> Supplementary</a> </p>
            </div>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>



<!-- abstract -->
<section>
  <div class="container">
    <div class="row">
      <div class="col-12 text-center">
        <h3>Abstract</h3>
        <hr style="margin-top:0px">
        <img class="img-fluid" src="images/abstract.jpg" alt="Architechture">
        <br><br>
        <p class="text-left"> 3D representation and reconstruction of human bodies have been studied for a long time in
          computer vision. Traditional methods rely mostly on parametric statistical linear models, limiting the space
          of possible bodies to linear combinations. It is only recently that some approaches try to leverage neural
          implicit representations for human body modeling, and while demonstrating impressive results, they are either
          limited by representation capability or not physically meaningful and controllable. In this work, we propose a
          novel neural implicit representation for the human body, which is fully differentiable and optimizable with
          disentangled shape and pose latent spaces. Contrary to prior work, our representation is designed based on the
          kinematic model, which makes the representation controllable for tasks like pose animation, while
          simultaneously allowing the optimization of shape and pose for tasks like 3D fitting and pose tracking. Our
          model can be trained and fine-tuned directly on non-watertight raw data with well-designed losses. Experiments
          demonstrate the improved 3D reconstruction performance over SoTA approaches and show the applicability of our
          method to shape interpolation, model fitting, pose tracking, and motion retargeting.</p>
      </div>
    </div>
  </div>
</section>
<br>




<!-- framework -->
<section>
  <div class="container">
    <div class="row">
      <div class="col-12 text-center">
        <h3>Framework Overview</h3>
        <hr style="margin-top:0px">
        <img class="img-fluid" src="images/framework.jpg" alt="Architechture">
        <hr>
        <p class="text-justify"> For learning SDF functions per body part, we use a piecewise deformable model
          conditioned on a shape code $z_{s}$, a per-joint feature $z_{p}^{b}$ describing pose-dependent deformations
          and canonical query points $x^{i}$. The bone transformations needed for obtaining $z_{p}^{b}$ can be computed
          with SMPL pose joint rotations and skeleton joints in canonical space (i.e. T-pose). For the former, we adopt
          VPoser while for the latter, we introduce our novel VJointer module. The per-joint SDF predictions are
          combined with a SoftMin function to obtain the SDF values of the final mesh.
        </p>
      </div>
    </div>
  </div>
</section>
<br>


<!-- overview video -->
<section>
  <div class="container">
    <div class="row">
      <div class="col-12 text-center">
        <h3>2-Minutes Introduction</h3>
        <hr style="margin-top:0px">
        <div class="embed-responsive embed-responsive-16by9">
          <iframe style="clip-path: inset(1px 1px)" width="100%" height="100%"
            src="https://www.youtube.com/embed/6hx_TP1X4ck" frameborder="0"
            allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
        </div>
      </div>
    </div>
  </div>
</section>
<br><br>


<!-- Applications -->
<section>
  <div class="container">
    <div class="row">
      <div class="col-12 text-center">
        <h3>Applications</h3>
        <hr style="margin-top:0px">


        <!-- Shape Swapping and Pose Animation -->
        <h4>Shape Swapping and Pose Animation</h4>
        <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
          <source src="images/swapping.mp4" type="video/mp4">
        </video>
        <p class="text-justify">
          By exchanging shape latent codes, we can swap between different shape identities.
          By exchanging pose parameters, we can also animate one subject with the novel poses taken from others.
        </p>
        <br>

        <!-- Pose Tracking and Motion Retargeting -->
        <h4>Pose Tracking and Motion Retargeting</h4>
        <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
          <source src="images/tracking.mp4" type="video/mp4">
        </video>
        <p class="text-justify">
          Given a sparse point cloud (shown on the top), we can optimize over the pose latent space and retarget the
          tracked motion to cartoon characters (on the bottom right).
        </p>
        <br>

        <!-- Fine-tuning on Raw Scans -->
        <h4>Fine-tuning on Raw Scans</h4>
        <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
          <source src="images/finetune.mp4" type="video/mp4">
        </video>
        <p class="text-justify">
          Thanks to our non-rigid geometric supervision with self-supervised losses, we can fine-tune LatentHuman on the
          DFaust raw scan dataset or even on the CAPE clothed human dataset.
        </p>
        <br>

        <!-- Representation Comparison on AMASS/DFaust&MoVi -->
        <h4>Representation Comparison</h4>
        <div class="row">
          <div class="col text-center ">
            <h5>AMASS/DFaust</h5>
            <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
              <source src="images/amass.mp4" type="video/mp4">
            </video>
          </div>

          <!-- Representation Comparison on AMASS/MoVi -->
          <div class="col text-center ">
            <h5>AMASS/MoVi</h5>
            <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
              <source src="images/movi.mp4" type="video/mp4">
            </video>
          </div>
          <p class="text-justify">
            We compare our method with NASA and LEAP on the AMASS-DFaust and the AMASS-MoVi dataset.
            Our LatentHuman preserves details better while avoiding blend skinning artifacts, e.g., when non-adjacent
            body parts come closer together where the hands are moved close to the head or in complex
            situations like cross-legged sitting.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<br>


<!-- 10min Talk -->
<section>
  <div class="container">
    <div class="row">
      <div class="col-12 text-center">
        <h3>10-minutes Talk</h3>
        <hr style="margin-top:0px">
        <div class="embed-responsive embed-responsive-16by9">
          <iframe style="clip-path: inset(1px 1px)" width="100%" height="100%"
            src="https://www.youtube.com/embed/IejsyGZocDQ" frameborder="0"
            allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
        </div>
      </div>
    </div>
  </div>
</section>
<br><br>

<!-- citing -->
<section>
  <div class="container">
    <div class="row ">
      <div class="col-12">
        <h3>Citation</h3>
        <hr style="margin-top:0px">
        <pre style="background-color: #e9eeef;padding: 1.25em 1.5em">
		<code>
@inproceedings{lombardi2021latenthuman,
    Title = {LatentHuman: Shape-and-Pose Disentangled Latent Representation for for Human Bodies},
    Author = {Sandro, Lombardi and Bangbang, Yang and Tianxing, Fan and Hujun, Bao and Guofeng, Zhang and Marc, Pollefeys and Zhaopeng, Cui},
    Year = {2021},
    journal = {International Conference on 3D Vision (3DV)},
}
  </code></pre>
        <hr>
      </div>
    </div>
  </div>
</section>


<footer class="text-center" style="margin-bottom:10px">
  Thanks to <a href="https://lioryariv.github.io/" target="_blank">Lior Yariv</a> for the website template.
</footer>

<script>
  MathJax = {
    tex: { inlineMath: [['$', '$'], ['\\(', '\\)']] }
  };
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-SXH5LVV7F0"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() { dataLayer.push(arguments); }
  gtag('js', new Date());

  gtag('config', 'G-SXH5LVV7F0');
</script>

</body>

</html>
